{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e1b8c6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoTokenizer\n",
    "from guidellm import GenerativeTextScenario\n",
    "from guidellm.benchmark.entrypoints import benchmark_with_scenario # Import for running the benchmark\n",
    "\n",
    "# Assuming JSON is available for display, e.g., from IPython.display\n",
    "# If not in an IPython environment, define a simple print function for JSON\n",
    "try:\n",
    "    from IPython.display import JSON\n",
    "except ImportError:\n",
    "    import json\n",
    "    def JSON(data):\n",
    "        print(json.dumps(data, indent=2))\n",
    "\n",
    "# --- Configuration for the model and tokenizer ---\n",
    "# IMPORTANT: Use the full Hugging Face model ID for the tokenizer\n",
    "# This ensures we load the correct pre-trained tokenizer from the Hugging Face Hub.\n",
    "# Based on previous context, \"RedHatAI/Qwen2.5-7B-quantized.w8a8\" is the expected ID.\n",
    "QWEN_MODEL_OR_TOKENIZER_ID = \"RedHatAI/Qwen2.5-7B-quantized.w8a8\"\n",
    "# The 'model' parameter in GenerativeTextScenario might expect a simpler name or the full ID\n",
    "# depending on how your server is configured. Using the name \"qwen25\" as per your original code.\n",
    "MODEL_NAME_FOR_SERVER = \"qwen25\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Attempting to load tokenizer for: {QWEN_MODEL_OR_TOKENIZER_ID}\")\n",
    "\n",
    "    # --- Step 1: Pre-load the tokenizer ---\n",
    "    # This ensures that 'guidellm' receives a valid tokenizer object,\n",
    "    # avoiding the 'Repo id must be in the form...' error.\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(QWEN_MODEL_OR_TOKENIZER_ID)\n",
    "        print(\"Tokenizer loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tokenizer: {e}\")\n",
    "        print(\"Please ensure the model name is correct and you have an active internet connection if downloading.\")\n",
    "        print(\"Also, verify that the necessary Python packages (e.g., 'transformers', 'sentencepiece') are installed.\")\n",
    "        tokenizer = None # Set to None if loading fails, so scenario creation can be skipped or handled.\n",
    "\n",
    "    if tokenizer:\n",
    "        # --- Step 2: Configure the GenerativeTextScenario ---\n",
    "        scenario = GenerativeTextScenario(\n",
    "            # The hostname of our model server\n",
    "            target=\"https://qwen25-innovatech.apps.cluster-m9cgf.m9cgf.sandbox2631.opentlc.com/v1\",\n",
    "            \n",
    "            # The name of the model we wish to test on the server.\n",
    "            model=MODEL_NAME_FOR_SERVER,\n",
    "            \n",
    "            # The data processor (tokenizer) of our model.\n",
    "            # We now pass the pre-loaded tokenizer OBJECT directly.\n",
    "            processor=tokenizer,\n",
    "\n",
    "            # Configure our load pattern and define a number of \"rates\".\n",
    "            rate_type=\"concurrent\",\n",
    "            rate=[1, 2, 4, 16, 64],\n",
    "\n",
    "            # For each rate: run a 30 second benchmark with no limit on the number of requests.\n",
    "            max_seconds=30,\n",
    "            max_requests=None,\n",
    "\n",
    "            # --- FIX: `data` parameter reverted to string format for synthetic dataset config ---\n",
    "            # guidellm's SyntheticDatasetConfig.parse_str() expects a string.\n",
    "            data=\"prompt_tokens=256,prompt_tokens_stdev=32,output_tokens=128\",\n",
    "        )\n",
    "\n",
    "        # Display our scenario\n",
    "        print(\"\\n--- Generated Scenario Configuration ---\")\n",
    "        JSON(scenario.model_dump())\n",
    "\n",
    "        print(\"\\n--- Step 3: Run the benchmark ---\")\n",
    "        try:\n",
    "            # The benchmark_with_scenario function is an async function,\n",
    "            # so it needs to be awaited. This script assumes it's being run\n",
    "            # in an async context (e.g., Jupyter notebook with an event loop).\n",
    "            # If running as a standalone script, you might need to use asyncio.run(main_function()).\n",
    "            results, _ = await benchmark_with_scenario(scenario, output_path=None, output_extras=None)\n",
    "            print(\"\\nBenchmark completed successfully!\")\n",
    "            # You can now process 'results' here if needed\n",
    "            # For example: print(results)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError during benchmark execution: {e}\")\n",
    "            print(\"\\nPossible reasons for failure:\")\n",
    "            print(\"1. Network connectivity issue to the target server:\")\n",
    "            print(f\"   - Please check if '{scenario.target}' is reachable from your environment.\")\n",
    "            print(\"   - Verify firewalls or proxy settings if applicable.\")\n",
    "            print(\"2. Model server issues:\")\n",
    "            print(f\"   - The model '{scenario.model}' might not be correctly deployed or accessible on the server.\")\n",
    "            print(\"   - The server might be overloaded or down.\")\n",
    "            print(\"   - Check the server logs for more specific error messages.\")\n",
    "            print(\"3. Authentication required:\")\n",
    "            print(\"   - If your model server requires authentication (e.g., API key, Hugging Face token),\")\n",
    "            print(\"     ensure `guidellm` is configured to provide it. This typically involves passing\")\n",
    "            print(\"     a `token` or `backend_args` to GenerativeTextScenario, if supported by guidellm.\")\n",
    "            print(\"4. Incorrect server endpoint or model name:\")\n",
    "            print(\"   - Double-check the `target` URL and `model` name against your server's documentation.\")\n",
    "            print(\"5. Insufficient resources on the server to handle the load specified by 'rate'.\")\n",
    "            print(\"\\nIf the error persists, consider debugging 'guidellm.dataset.in_memory.py' directly if possible,\")\n",
    "            print(\"or reviewing the guidellm documentation for how the 'data' parameter should be formatted for synthetic data.\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"\\nSkipping GenerativeTextScenario creation and benchmark execution due to tokenizer loading failure.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
